{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing\n",
    "\n",
    "Data pre-processing and exploratory analysis of restaurant inspections data before modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bkley\\Anaconda3\\lib\\site-packages\\statsmodels\\compat\\pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import distance\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.api import OLS\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "PATH_YELP_DATA = '../data/yelp_data.csv'\n",
    "PATH_INSPECTIONS = '../data/nyc_restaurant_inspection_data.csv'\n",
    "PATH_DEMOGRAPHICS = '../data/demographics_zipcode.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_inspections (url=PATH_INSPECTIONS):\n",
    "    \"\"\"Creates DF from NYC inspections data and handles preliminary\n",
    "    data-cleaning. Sets CAMIS as index.\n",
    "\n",
    "    Raises:\n",
    "        RuntimeError if number of rows after cleaning data are fewer than 150K.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(url)\n",
    "    # Remove records not containing a zip code and ensure zip code is an int.\n",
    "    # df = df[df['ZIPCODE'].notnull()]\n",
    "    df['ZIPCODE'] = df['ZIPCODE'].astype(np.int64)\n",
    "    if len(df.index) < 150000:\n",
    "        raise RuntimeError('Inspections DF contains < 150,000 rows.')\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_yelp (url=PATH_YELP_DATA):\n",
    "    \"\"\"Sets CAMIS as index.\"\"\"\n",
    "    df = pd.read_csv(url, encoding='ISO-8859-1')\n",
    "    df['zip_code'] = df['zip_code'].astype(np.int64)\n",
    "    df.set_index('CAMIS', inplace=True, drop=False)\n",
    "    if len(df.index) < 9500:\n",
    "        raise RuntimeError('Yelp DF contains < 9,500 rows.')\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_complete_data ():\n",
    "    inspections = load_inspections()\n",
    "    yelp = load_yelp()\n",
    "    result = pd.merge(inspections, yelp, how='left', on='CAMIS')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = load_complete_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations: 157955\n",
      "Number of predictors: 33\n"
     ]
    }
   ],
   "source": [
    "# Check dimensions of dataset\n",
    "n, p = data.shape\n",
    "print(f\"Number of observations: {n}\")\n",
    "print(f\"Number of predictors: {p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CAMIS                      int64\n",
       "DBA                       object\n",
       "BORO                      object\n",
       "BUILDING                  object\n",
       "STREET                    object\n",
       "ZIPCODE                    int64\n",
       "PHONE                     object\n",
       "CUISINE.DESCRIPTION       object\n",
       "INSPECTION.DATE           object\n",
       "ACTION                    object\n",
       "VIOLATION.CODE            object\n",
       "VIOLATION.DESCRIPTION     object\n",
       "CRITICAL.FLAG             object\n",
       "SCORE                    float64\n",
       "GRADE                     object\n",
       "GRADE.DATE                object\n",
       "RECORD.DATE                int64\n",
       "INSPECTION.TYPE           object\n",
       "id                        object\n",
       "name                      object\n",
       "url                       object\n",
       "phone                    float64\n",
       "latitude                 float64\n",
       "longitude                float64\n",
       "review_count               int64\n",
       "price                    float64\n",
       "rating                   float64\n",
       "transactions              object\n",
       "categories                object\n",
       "address                   object\n",
       "city                      object\n",
       "state                     object\n",
       "zip_code                   int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Column names and types\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update column names with \"_\" separator\n",
    "data.columns = [c.replace(\".\", \"_\").lower() for c in data.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "camis: 9509\n",
      "zipcode: 196\n",
      "violation_code: 98\n",
      "cuisine_description: 83\n",
      "transactions: 13\n",
      "categories: 3446\n"
     ]
    }
   ],
   "source": [
    "# Number of unique values for specified columns\n",
    "cols = ['camis', 'zipcode', 'violation_code', 'cuisine_description', 'transactions', 'categories']\n",
    "for c in cols:\n",
    "    cnt = data[c].nunique()\n",
    "    print(\"{}: {}\".format(c, cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fix date formats \n",
    "from datetime import date, timedelta\n",
    "def get_excel_date(row):\n",
    "    if row==row:\n",
    "        try:\n",
    "            if len(row)==5:\n",
    "                return date(1900, 1, 1) + timedelta(int(row))\n",
    "            else:\n",
    "                return pd.to_datetime(row).strftime('%Y-%m-%d')\n",
    "        except:\n",
    "            None\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix formats of date columns\n",
    "date_cols = ['inspection_date', 'grade_date', 'record_date']\n",
    "for col in date_cols:\n",
    "    data[col] = data[col].apply(get_excel_date)\n",
    "    \n",
    "# Create inspection date variables\n",
    "data['inspection_year'] = pd.DatetimeIndex(data['inspection_date']).year\n",
    "data['inspection_month'] = pd.DatetimeIndex(data['inspection_date']).month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique observations: 52472\n"
     ]
    }
   ],
   "source": [
    "# Drop observations with no score \n",
    "data = data[pd.notnull(data.score)]\n",
    "\n",
    "# Drop observations with a negative score\n",
    "data = data[data.score >= 0]\n",
    "      \n",
    "# Replace missing values with \"NULL\" \n",
    "#   This causes issues when doing group by below\n",
    "data.fillna(\"NULL\", inplace=True)\n",
    "\n",
    "# Number of unique observations (restaurant, inspection date)\n",
    "count = len(data[[\"camis\", \"inspection_date\"]].drop_duplicates())\n",
    "print(\"Number of unique observations: {}\".format(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cols = ['camis',\n",
    "              'dba',\n",
    "              'boro',\n",
    "              'zipcode',\n",
    "              'cuisine_description',\n",
    "              'inspection_date',\n",
    "              'inspection_year',\n",
    "              'inspection_month',\n",
    "              'score',\n",
    "              'grade',\n",
    "              'name',\n",
    "              'latitude',\n",
    "              'longitude',\n",
    "              'review_count',\n",
    "              'price',\n",
    "              'rating',\n",
    "              'transactions',\n",
    "              'categories',\n",
    "              'city']\n",
    "\n",
    "# Group violations such that each observation in data is an individual inspection\n",
    "grp = data.groupby(model_cols, group_keys=True)['violation_code'].apply(list)\n",
    "data = pd.DataFrame(grp.reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure observations are unique by camis and inspection date\n",
    "# Take observation with lowest score if multiple entries exist\n",
    "\n",
    "# Rank duplicate observations by score\n",
    "g = data.sort_values(['camis', 'inspection_date', 'score']).groupby(['camis', 'inspection_date'])\n",
    "data['RNK'] = g['score'].rank(method='first')\n",
    "\n",
    "# Drop duplicates\n",
    "n = len(data)\n",
    "data = data.loc[data.RNK == 1, :]\n",
    "data.drop('RNK', axis=1, inplace=True)\n",
    "\n",
    "# Set index\n",
    "data.set_index(['camis', 'inspection_date'], drop=False, verify_integrity=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cuisine description**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_cuisine_format(row):\n",
    "\n",
    "    mapping = {\n",
    "        'Bottled beverages, including water, sodas, juices, etc.': 'Bottled_beverages',\n",
    "        'CafÃ©/Coffee/Tea': 'Coffee_Tea',\n",
    "        'Ice Cream, Gelato, Yogurt, Ices': 'IceCream_Gelato',\n",
    "        'Juice, Smoothies, Fruit Salads': 'Juice_Smoothies',\n",
    "        'Latin (Cuban, Dominican, Puerto Rican, South & Central American)': 'Latin',\n",
    "        'Sandwiches/Salads/Mixed Buffet': 'Sandwiches',\n",
    "        'Soups & Sandwiches': 'Sandwiches',\n",
    "        'Tex-Mex': 'TexMex'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        row = mapping[row]\n",
    "    except:\n",
    "        row = row.strip().replace(\"/\", \"_\").replace(\" \", \"_\")\n",
    "    \n",
    "    return row\n",
    "        \n",
    "data['cuisine_description'] = data['cuisine_description'].apply(fix_cuisine_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_list_format(row):\n",
    "    \n",
    "    row = str(row)\n",
    "    \n",
    "    chars = ''''\"[]'''\n",
    "    for c in chars:\n",
    "        row = row.strip().replace(c, \"\")\n",
    "    row = row.replace(\",\", \"|\").replace(\" \", \"\")\n",
    "    \n",
    "    return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transactions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean values\n",
    "data['transactions'] = data['transactions'].apply(fix_list_format)\n",
    "\n",
    "# Get unique set of transactions\n",
    "transactions = set()\n",
    "for s in data.transactions:\n",
    "    transactions.update(s.split('|'))\n",
    "\n",
    "# Create separate columns for each unique value\n",
    "for txn in transactions:\n",
    "    data[\"trans_\" + txn] = [1 if txn in t.split('|') else 0 for t in data.transactions]\n",
    "    \n",
    "# Drop column\n",
    "data.drop('transactions', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Categories**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of category combinations: 3423\n",
      "Total number of unique categories: 309\n"
     ]
    }
   ],
   "source": [
    "# Clean up categories column\n",
    "data['categories'] = data['categories'].apply(fix_list_format)\n",
    "print(\"Total number of category combinations: {}\".format(data.categories.nunique()))\n",
    "\n",
    "# Get set of unique categories\n",
    "categories = set()\n",
    "for s in data.categories:\n",
    "    categories.update(s.split('|'))\n",
    "print(\"Total number of unique categories: {}\".format(len(categories)))\n",
    "\n",
    "# Create column for each unique category and encode \n",
    "#    Might need to refine this later to reduce dimensionality\n",
    "for cat in categories:\n",
    "    data[\"cat_\" + cat] = [1 if cat in c.split('|') else 0 for c in data.categories]\n",
    "    \n",
    "# Drop column\n",
    "data.drop('categories', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Violations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of violation combinations: 17796\n",
      "Total number of unique violations: 70\n"
     ]
    }
   ],
   "source": [
    "# Clean up categories column\n",
    "data['violation_code'] = data['violation_code'].apply(fix_list_format)\n",
    "print(\"Total number of violation combinations: {}\".format(data.violation_code.nunique()))\n",
    "\n",
    "# Get set of unique categories\n",
    "violations = set()\n",
    "for s in data.violation_code:\n",
    "    violations.update(s.split('|'))\n",
    "print(\"Total number of unique violations: {}\".format(len(violations)))\n",
    "\n",
    "# Create column for each unique category and encode \n",
    "#    Might need to refine this later to reduce dimensionality\n",
    "for vcode in violations:\n",
    "    data[\"violation_\" + vcode] = [1 if vcode in v.split('|') else 0 for v in data.violation_code]\n",
    "    \n",
    "# Drop column\n",
    "data.drop('violation_code', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Restaurant name**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of restuarant name in inspection data\n",
    "data[\"restaurant_name_len\"] = data[\"dba\"].apply(len)\n",
    "\n",
    "# Length of restaurant name in Yelp data\n",
    "data[\"restaurant_name_len_yelp\"] = data[\"name\"].apply(len)\n",
    "\n",
    "# Similarity of names in inspection data and Yelp data\n",
    "data[\"restaurant_name_sim\"] = data.apply(lambda row: distance.levenshtein(row[\"dba\"], row[\"name\"]), axis=1)\n",
    "\n",
    "# Number of observations by restaurant \n",
    "df = data.dba.value_counts().reset_index()\n",
    "df.columns = ['dba', 'restaurant_name_count']\n",
    "data = pd.merge(data, df, how='left', on='dba')\n",
    "\n",
    "# Create initial flag for restaurant chains (count > 50)\n",
    "data[\"restaurant_name_chain\"] = data['restaurant_name_count'].apply(lambda x: 1 if x > 50 else 0)\n",
    "\n",
    "# Drop column\n",
    "data.drop('name', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Demographics by zipcode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "cen_data = pd.read_csv(PATH_DEMOGRAPHICS)\n",
    "\n",
    "# Update column names\n",
    "cen_data.columns =  [\"cen_\" + c.replace(\" \", \"_\").lower() for c in cen_data.columns]\n",
    "cen_data.columns.values[0] = 'zipcode'\n",
    "\n",
    "# Merge onto base dataset\n",
    "data = pd.merge(data, cen_data, how='left', on='zipcode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Region**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Borough\n",
    "##########\n",
    "\n",
    "# Create dummy variables\n",
    "col = 'boro'\n",
    "df = pd.get_dummies(data[col], prefix=col)\n",
    "data = data.join(df)\n",
    "data.drop(col, axis=1, inplace=True)\n",
    "\n",
    "# City\n",
    "#########\n",
    "\n",
    "# Group areas with low volumes\n",
    "col = 'city'\n",
    "counts = list(pd.value_counts(data[col]).index[:30])\n",
    "data[col] = data[col].apply(lambda row: row if row in counts else 'Other')\n",
    "\n",
    "# Create dummy variables\n",
    "df = pd.get_dummies(data[col], prefix=col)\n",
    "data = data.join(df)\n",
    "data.drop(col, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Clean up column names (lowercase and no spaces)\n",
    "data.columns = [c.replace(\".\", \"_\").lower() for c in data.columns]\n",
    "\n",
    "# Replace NULL with NA again\n",
    "data.replace(\"NULL\", np.nan, inplace=True)\n",
    "\n",
    "# Impute missing values (categorical)\n",
    "cols = ['rating', 'price']\n",
    "imp = Imputer(missing_values=\"NaN\", strategy=\"most_frequent\")\n",
    "data[cols] = imp.fit_transform(data[cols])\n",
    "\n",
    "# Census values\n",
    "imp = Imputer(missing_values=\"NaN\", strategy=\"mean\")\n",
    "cols = [c for c in data.columns if c[:3] == \"cen\"]\n",
    "data[cols] = imp.fit_transform(data[cols])\n",
    "\n",
    "# Reorder columns\n",
    "cols = list(data.columns)\n",
    "target_cols = ['grade', 'score']\n",
    "for c in target_cols:\n",
    "    cols.remove(c)\n",
    "    cols.append(c)\n",
    "\n",
    "data = data[cols]\n",
    "\n",
    "#Export data to csv\n",
    "#data.to_csv('data/model_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of score\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "ax.set_xlim(0, 80)\n",
    "ax.set_title(\"Distribution of score\")\n",
    "sns.distplot(data.score, kde=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of log score\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "ax.set_xlim(0, 6)\n",
    "ax.set_title(\"Distribution of log score\")\n",
    "sns.distplot(np.log(data.score+2), bins=30, kde=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of sqrt score\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "ax.set_xlim(0, 6)\n",
    "ax.set_title(\"Distribution of sqrt score\")\n",
    "sns.distplot(np.sqrt(data.score+1), bins=50, kde=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of rating\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "ax.set_title(\"Distribution of rating\")\n",
    "sns.countplot(x=\"rating\", data=data, color='green', alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of number of reviews\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "ax.set_title(\"Distribution of review count\")\n",
    "ax.set_xlim(0, 1000)\n",
    "sns.distplot(data.review_count, kde=False, bins=200, color='green');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of scores by rating\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "data['log_score'] = np.log(data['score']+2)\n",
    "sns.boxplot(x=\"rating\", y=\"log_score\", data=data, color='grey', saturation=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of scores by restaurant chain flag\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "sns.boxplot(x=\"restaurant_name_chain\", y=\"log_score\", data=data, color='grey', saturation=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of scores by restaurant chain flag\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "sns.boxplot(x=\"price\", y=\"log_score\", data=data, color='grey', saturation=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of scores by restaurant chain flag\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "sns.boxplot(x=\"inspection_month\", y=\"log_score\", data=data, color='grey', saturation=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.sample(500)\n",
    "\n",
    "# Distribution of scores by restaurant chain flag\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "ax.set_xlim(0, 1000)\n",
    "sns.regplot(x=\"review_count\", y=\"log_score\", data=df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.sample(500)\n",
    "\n",
    "# Distribution of scores by restaurant chain flag\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "ax.set_xlim(0, 30)\n",
    "sns.regplot(x=\"restaurant_name_len\", y=\"log_score\", data=df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's just look at couple of predictors to start**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cols = ['review_count', 'price', 'rating', 'log_score']\n",
    "\n",
    "# Get predictors and response\n",
    "df = data[model_cols]\n",
    "\n",
    "# Create train and test sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.5, random_state=87)\n",
    "\n",
    "# Model predictors\n",
    "X_train = train_df.iloc[:,:-1].values\n",
    "X_test = test_df.iloc[:,:-1].values\n",
    "\n",
    "X_train = sm.add_constant(X_train)\n",
    "X_test = sm.add_constant(X_test)\n",
    "\n",
    "# Response\n",
    "y_train = train_df.iloc[:,-1].values\n",
    "y_test = test_df.iloc[:,-1].values\n",
    "\n",
    "# Fit model\n",
    "mod = OLS(y_train, X_train)\n",
    "res = mod.fit()\n",
    "\n",
    "# Print summary statistics\n",
    "names = ['const'] + list(train_df.columns[:-1])\n",
    "print(res.summary(xname=names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Violation codes? Expected to be very correlated by construction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_cols = [c for c in data.columns if \"violation\" in c][:20]\n",
    "model_cols.append('log_score')\n",
    "\n",
    "# Get predictors and response\n",
    "df = data[model_cols]\n",
    "\n",
    "# Create train and test sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.5, random_state=87)\n",
    "\n",
    "# Model predictors\n",
    "X_train = train_df.iloc[:,:-1].values\n",
    "X_test = test_df.iloc[:,:-1].values\n",
    "\n",
    "X_train = sm.add_constant(X_train)\n",
    "X_test = sm.add_constant(X_test)\n",
    "\n",
    "# Response\n",
    "y_train = train_df.iloc[:,-1].values\n",
    "y_test = test_df.iloc[:,-1].values\n",
    "\n",
    "# Fit model\n",
    "mod = OLS(y_train, X_train)\n",
    "res = mod.fit()\n",
    "\n",
    "# Print summary statistics\n",
    "names = ['const'] + list(train_df.columns[:-1])\n",
    "print(res.summary(xname=names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Areas? Don't expect this to be very important**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cols = [c for c in data.columns if c[:4] == \"city\"]\n",
    "model_cols.append('log_score')\n",
    "\n",
    "# Get predictors and response\n",
    "df = data[model_cols]\n",
    "\n",
    "# Create train and test sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.5, random_state=87)\n",
    "\n",
    "# Model predictors\n",
    "X_train = train_df.iloc[:,:-1].values\n",
    "X_test = test_df.iloc[:,:-1].values\n",
    "\n",
    "X_train = sm.add_constant(X_train)\n",
    "X_test = sm.add_constant(X_test)\n",
    "\n",
    "# Response\n",
    "y_train = train_df.iloc[:,-1].values\n",
    "y_test = test_df.iloc[:,-1].values\n",
    "\n",
    "# Fit model\n",
    "mod = OLS(y_train, X_train)\n",
    "res = mod.fit()\n",
    "\n",
    "# Print summary statistics\n",
    "names = ['const'] + list(train_df.columns[:-1])\n",
    "print(res.summary(xname=names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's fit a LASSO model to subset the most important features\n",
    "\n",
    "first_index = data.columns.get_loc(\"review_count\")\n",
    "last_index = data.columns.get_loc(\"city_woodside\")\n",
    "\n",
    "model_cols = list(data.columns[first_index: last_index + 1])\n",
    "model_cols = [c for c in model_cols if \"violation\" not in c]\n",
    "model_cols.append('log_score')\n",
    "\n",
    "# Get predictors and response\n",
    "df = data[model_cols]\n",
    "\n",
    "# Model predictors\n",
    "X_train = df.iloc[:,:-1].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "# Response\n",
    "y_train = df.iloc[:,-1].values\n",
    "\n",
    "# Fit LASSO model\n",
    "shrinkage = [0.01]\n",
    "mod = LassoCV(alphas=shrinkage, cv=10)\n",
    "lasso = mod.fit(X_train, y_train)\n",
    "\n",
    "# Print summary statistics\n",
    "df = pd.DataFrame(lasso.coef_, columns=[\"Coefficient\"], index=model_cols[:-1])\n",
    "df[df.Coefficient != 0]\n",
    "\n",
    "rsq = r2_score(y_train, lasso.predict(X_train))\n",
    "print(\"Training R-squared: {0:.4f}\".format(rsq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_keep = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cuisine description**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group areas with low volumes\n",
    "col = 'cuisine_description'\n",
    "counts = list(pd.value_counts(data[col]).index[:30])\n",
    "data[col] = data[col].apply(lambda row: row if row in counts else 'Other')\n",
    "\n",
    "# Create dummy variables\n",
    "df = pd.get_dummies(data[col], prefix=col)\n",
    "data = data.join(df)\n",
    "data.drop(col, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Previous score / time since previous inspection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "data = data_keep.copy()\n",
    "\n",
    "# Join on previous scores\n",
    "df = pd.merge(data[['camis', 'inspection_date']],\n",
    "              data[['camis', 'inspection_date', 'score']], on='camis', suffixes=('', '_prev'))\n",
    "df = df.loc[df['inspection_date'] > df['inspection_date_prev'], :]\n",
    "\n",
    "# Months since previous inspection\n",
    "df['time_since_prev'] = df['inspection_date'] - df['inspection_date_prev']\n",
    "\n",
    "# Most recent score\n",
    "g = df.sort_values(['camis', 'inspection_date', 'time_since_prev']).groupby(['camis', 'inspection_date'])\n",
    "df['rnk'] = g['time_since_prev'].rank(method='first')\n",
    "\n",
    "# Average historic scores\n",
    "df.set_index(['camis', 'inspection_date'], inplace=True, drop=False)\n",
    "g = df.groupby(['camis', 'inspection_date'])['score']\n",
    "df['score_avg'] = g.mean()\n",
    "df['score_std'] = g.std()\n",
    "df['score_max'] = g.max()\n",
    "df['score_cnt'] = g.count()\n",
    "\n",
    "# Unique index\n",
    "df = df.loc[df['rnk'] == 1, :]\n",
    "df.drop('rnk', axis=1, inplace=True)\n",
    "\n",
    "# Join back onto base dataframe\n",
    "data = pd.merge(data, df[['camis', 'inspection_date', 'score', 'score_avg', 'score_std', \n",
    "                          'score_max', 'score_cnt', 'time_since_prev']], \n",
    "                how='left', on=['camis', 'inspection_date'], suffixes=('', '_prev'))\n",
    "\n",
    "# Fraction of year since previous visit\n",
    "data['time_since_prev'] = data['time_since_prev'].dt.days / 365\n",
    "\n",
    "# Set index\n",
    "data.set_index(['camis', 'inspection_date'], drop=False, verify_integrity=True, inplace=True)\n",
    "\n",
    "# Flag if first inspection\n",
    "data['first_inspection'] = data['time_since_prev'].apply(lambda row: 0 if row == row else 1)\n",
    "\n",
    "# Impute missing values\n",
    "cols = ['score_prev', 'score_avg', 'score_std', 'score_max', 'score_cnt', 'time_since_prev']\n",
    "imp = Imputer(missing_values=\"NaN\", strategy=\"mean\")\n",
    "data[cols] = imp.fit_transform(data[cols])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Restaurant category (Yelp data)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Large number of categories (~300) so drop categories\n",
    "# with observations lower than some threshold\n",
    "\n",
    "min_obs = 500\n",
    "\n",
    "cols = [c for c in data.columns if c[:3] == \"cat\"]\n",
    "for c in cols:\n",
    "    if data[c].sum() < min_obs:\n",
    "        data.drop(c, axis=1, inplace=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up column names (lowercase and no spaces)\n",
    "data.columns = [c.replace(\".\", \"_\").lower() for c in data.columns]\n",
    "\n",
    "# Reorder columns\n",
    "cols = list(data.columns)\n",
    "target_cols = ['grade', 'score', 'log_score']\n",
    "for c in target_cols:\n",
    "    cols.remove(c)\n",
    "    cols.append(c)\n",
    "data = data[cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export data to csv\n",
    "#data.to_csv('data/model_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
